# -*- coding: utf-8 -*-
"""CS779-CP-Ashutosh-Tripathi-220242.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gLFxkDBmcKreWBmMepezXvOfr549ROpk
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import json
import numpy as np
import re
import string
import nltk
import pandas as pd
import matplotlib.pyplot as plt

nltk.download('punkt_tab')

import json
# path to the dataset on Kaggle
train_path = '/kaggle/input/train-data/train_data1.json'
with open(train_path, 'r') as file:
    data1 = json.load(file)

test_path = '/kaggle/input/test-data2/test_data1_final.json'
with open(test_path, 'r') as file:
    data2 = json.load(file)

for lang_pair in data1.keys():
  print(f"Language Pair: {lang_pair}")

for lang_pair in data2.keys():
  print(f"Language Pair: {lang_pair}")

train_inp = []
train_out = []
test_inp = []
test_out = []
train_id = []
val_id = []

# Train Data collections
for lang_pair, lang_data in data1.items():
    if(lang_pair == "English-Hindi"):
      print(f"Language Pair: {lang_pair}")
      for d_type, cont in lang_data.items():
          print(f"Data Type: {d_type}")
          for id, data in cont.items():
              source = data["source"]
              target = data["target"]
              train_inp.append(source)
              train_out.append(target)
              train_id.append(id)
# Test Data collections
for lang_pair, lang_data in data2.items():
    if(lang_pair == "English-Hindi"):
      print(f"Language Pair: {lang_pair}")
      for d_type, cont in lang_data.items():
          print(f"Data Type: {d_type}")
          for id, data in cont.items():
              source = data["source"]
              test_inp.append(source)

print(len(train_inp))
print(len(train_out))
print(len(test_inp))
print(len(test_out))

from collections import Counter

len_sen_source = []
for sen in train_inp:
  len_sen_source.append(len(sen))
len_sen_cnt = Counter(len_sen_source)

len_sen_cnt.most_common(10)

x={'English':train_inp,'Hindi':train_out}
df=pd.DataFrame(x)
df.head()

# fxn- it preprocess and remove punctuation and digit from sentence
def fxn(sen):
    res = ""
    for ch in sen:
        if ch not in string.punctuation and not ch.isdigit():
            res += ch
    return res

def preprocess(data):
    tok_sen = []
    for sen in data:
        sen = sen.lower()
        sen = fxn(sen)
        tok = nltk.word_tokenize(sen)
        tok_sen.append(tok)
    return tok_sen

en_train = preprocess(train_inp)
en_test=preprocess(test_inp)
de_train = preprocess(train_out)
de_test=preprocess(test_out)

en_train[0]

from tqdm import tqdm

en_idx2_word = ["<PAD>", "<SOS>", "<EOS>"]
de_idx2_word = ["<PAD>", "<SOS>", "<EOS>"]
# for en_train
for sent in en_train:
    for token in sent:
        if token not in en_idx2_word:
            en_idx2_word.append(token)

# for en_test
for sent in en_test:
    for token in sent:
        if token not in en_idx2_word:
            en_idx2_word.append(token)

for sent in de_train:
    for token in sent:
        if token not in de_idx2_word:
            de_idx2_word.append(token)

# # For de_test
# for sent in de_test:
#     for token in sent:
#         if token not in de_idx2_word:
#             de_idx2_word.append(token)

len(en_idx2_word)

de_idx2_word[-8:-2]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

en_word2_idx = {}
de_word2_idx = {}
for i in range(len(en_idx2_word)):
  en_word2_idx[en_idx2_word[i]] = i

for i in range(len(de_idx2_word)):
  de_word2_idx[de_idx2_word[i]] = i

max_len_en = 0
en_lengths = 0
for sen in en_train:
  max_len_en = max(max_len_en,len(sen))
  en_lengths+=len(sen)
en_lengths  = en_lengths/len(en_train)
max_len_en

tot_en_len = 0
en_maxi  = 0
for sen in en_train:
    tot_en_len += len(sen)
    en_maxi = max(en_maxi,len(sen))
en_avg = tot_en_len / len(en_train)

tot_de_len = 0
de_maxi = 0
for sen in de_train:
    tot_de_len += len(sen)
    de_maxi = max(de_maxi,len(sen))
de_avg = tot_de_len / len(de_train)

de_avg

en_avg

print(en_maxi,de_maxi)

seq_len = 75

def enc_pad(vocab, sen, maxi):
    sos = [vocab["<SOS>"]]
    eos = [vocab["<EOS>"]]
    pad = [vocab["<PAD>"]]

    if len(sen) < maxi - 2:
        n_pads = maxi - 2 - len(sen)
        enc = []
        for w in sen:
            enc.append(vocab[w])
        return sos + enc + eos + pad * n_pads
    else:
        enc = []
        for w in sen:
            enc.append(vocab[w])
        enc = enc[:maxi - 2]
        return sos + enc + eos

en_train_enc = []
for sent in en_train:
    enc = enc_pad(en_word2_idx, sent, seq_len)
    en_train_enc.append(enc)

en_test_enc = []
for sent in en_test:
    enc = enc_pad(en_word2_idx, sent, seq_len)
    en_test_enc.append(enc)

de_train_enc = []
for sent in de_train:
    enc = enc_pad(de_word2_idx, sent, seq_len)
    de_train_enc.append(enc)

# de_test_enc = []
# for sent in de_test:
#     enc = enc_pad(de_word2_idx, sent, seq_len)
#     de_test_enc.append(enc)

print(en_train_enc[8])

batch_size = 32

train_x = np.array(en_train_enc)
train_y = np.array(de_train_enc)
test_x = np.array(en_test_enc)
# test_y = np.array(de_test_enc)

train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))
test_ds = TensorDataset(torch.from_numpy(test_x))


train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)
#test_dl = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)

import torch
import torch.nn as nn

class EncoderBiLSTM(nn.Module):
    def __init__(self, inps, hs, emb_len=128, num_lay=3):
        super(EncoderBiLSTM, self).__init__()
        self.num_lay = num_lay
        self.hs = hs
        self.embed = nn.Embedding(inps, emb_len)
        self.lstm = nn.LSTM(
            emb_len, hs, num_layers=num_lay,
            batch_first=True, bidirectional=True
        )

    def forward(self, inp, hid, cell):
        embed = self.embed(inp)
        output, (hidden, cell) = self.lstm(embed, (hid, cell))
        return output, hidden, cell

    def ini_hid(self, batch_size, device):
        hid = torch.zeros(self.num_lay * 2, batch_size, self.hs, device=device)
        cell = torch.zeros(self.num_lay * 2, batch_size, self.hs, device=device)
        return hid, cell

import torch
import torch.nn as nn
import torch.nn.functional as F

class DecoderBiLSTM(nn.Module):
    def __init__(self, hs, out_s, emb_len=128, num_lay=3):
        super(DecoderBiLSTM, self).__init__()
        self.hs = hs
        self.num_layers = num_lay
        self.emb = nn.Embedding(out_s, emb_len)
        enc_hs = hs * 2
        # Bahdanu Attention
        self.w_h = nn.Linear(enc_hs, hs)
        self.w_s = nn.Linear(hs, hs)
        self.v = nn.Linear(hs, 1, bias=False)

        self.lstm = nn.LSTM(emb_len + enc_hs, hs, num_layers=num_lay, batch_first=True)
        self.out = nn.Linear(hs + enc_hs, out_s)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, inp, hid, cell, enc_out):
        emb = self.emb(inp)  # (batch, 1, emb_len)
        dec_hid = hid[-1].unsqueeze(1)  # (batch, 1, hs)

        x = torch.tanh(self.w_h(enc_out) + self.w_s(dec_hid))
        scores = self.v(x).squeeze(2)
        attn_weights = F.softmax(scores, dim=1)

        context = torch.bmm(attn_weights.unsqueeze(1), enc_out)

        lstm_input = torch.cat((emb, context), dim=2)
        out, (hid, cell) = self.lstm(lstm_input, (hid, cell))

        out = out.squeeze(1)
        context = context.squeeze(1)

        out = self.out(torch.cat((out, context), dim=1))
        out = self.softmax(out)

        return out, hid, cell

# define hidden_size
hidden_size = 128

encoder = EncoderBiLSTM(len(en_idx2_word), hidden_size).to(device)
decoder = DecoderBiLSTM(hidden_size, len(de_idx2_word)).to(device)

encoder

decoder

criterion = nn.NLLLoss()
enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-3)
dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-3)

losses = []

input_length = target_length = seq_len
SOS = en_word2_idx["<SOS>"]
EOS = en_word2_idx["<EOS>"]

epochs = 5
import random

import torch
import random
# setting teacher force ratio to 1
tfr = 1

for epoch in range(epochs):
    for idx, batch in enumerate(train_dl):
        batch_size = batch[0].size(0)
        enc_hid, enc_cell = encoder.ini_hid(batch_size, device)

        inp_ten = batch[0].to(device)
        tar_ten = batch[1].to(device)
        tar_len = tar_ten.size(1)

        enc_optimizer.zero_grad()
        dec_optimizer.zero_grad()

        enc_out, enc_hid, enc_cell = encoder(
            inp_ten, enc_hid, enc_cell
        )

        def merge_bidir(hidden):
            hidden = hidden.view(encoder.num_lay, 2, batch_size, encoder.hs)
            return torch.tanh(hidden[:, 0, :, :] + hidden[:, 1, :, :])

        dec_hid = merge_bidir(enc_hid)
        dec_cell = merge_bidir(enc_cell)

        dec_result = torch.zeros(batch_size, tar_len, len(de_idx2_word), device=device)
        dec_inp = tar_ten[:, 0].unsqueeze(1)

        for t in range(1, tar_len):
            decoder_output, dec_hid, dec_cell = decoder(
                dec_inp, dec_hid, dec_cell, enc_out
            )

            dec_result[:, t, :] = decoder_output

            # teacher forcing
            use = random.random() < tfr
            if use:
                dec_inp = tar_ten[:, t].unsqueeze(1)
            else:
                top1 = decoder_output.argmax(1)
                dec_inp = top1.unsqueeze(1)

        scores = dec_result[:, 1:, :].reshape(-1, dec_result.shape[2])
        targets = tar_ten[:, 1:].reshape(-1)
        loss = criterion(scores, targets)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1)
        torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1)
        enc_optimizer.step()
        dec_optimizer.step()

        losses.append(loss.item())
        if idx % 100 == 0:
            avg_loss = sum(losses) / len(losses)
            print(f"Epoch {epoch+1}, Batch {idx}, Avg Loss: {avg_loss:.4f}")

# checkpoint = {
#     "encoder_state": encoder.state_dict(),
#     "decoder_state": decoder.state_dict(),
#     "enc_optimizer": enc_optimizer.state_dict(),
#     "dec_optimizer": dec_optimizer.state_dict(),
#     "epoch": epoch,
#     "de_idx2_word": de_idx2_word,
#     "en_word2_idx": en_word2_idx,
# }

# torch.save(checkpoint, "seq2seq_model2.pkl")
# print("Model Saved")

# import zipfile

# zip_name = "seq2seq_model2.zip"
# with zipfile.ZipFile(zip_name, 'w') as zipf:
    # zipf.write("seq2seq_model2.pkl")

# from IPython.display import FileLink
# FileLink("seq2seq_model2.zip")

# checkpoint = torch.load("/kaggle/input/trained-model/seq2seq_model2.pkl", map_location=device)

# encoder.load_state_dict(checkpoint["encoder_state"])
# decoder.load_state_dict(checkpoint["decoder_state"])

# enc_optimizer.load_state_dict(checkpoint["enc_optimizer"])
# dec_optimizer.load_state_dict(checkpoint["dec_optimizer"])

# print("Model Loaded")

import matplotlib.pyplot as plt

plt.plot(losses)

from tqdm import tqdm

val_ids = [ i for i,_ in data2["English-Hindi"]["Test"].items()]

from tqdm import tqdm
import torch

val_outs = []

for i in tqdm(range(len(test_ds))):
    enc_hid, enc_cell = encoder.ini_hid(batch_size=1, device=device)

    inp_ten = test_ds[i][0].unsqueeze(0).to(device)  # shape (1, seq_len)
    result = []

    with torch.no_grad():
        enc_out, enc_hid, enc_cell = encoder(
            inp_ten, enc_hid, enc_cell
        )

        def merge_bidir(hidden):
            hidden = hidden.view(encoder.num_lay, 2, 1, encoder.hs)
            return torch.tanh(hidden[:, 0, :, :] + hidden[:, 1, :, :])

        dec_hid = merge_bidir(enc_hid)
        dec_cell = merge_bidir(enc_cell)
        dec_inp = torch.tensor([SOS], device=device).unsqueeze(1)  # shape (1,1)

        for di in range(1, tar_len):
            decoder_output, dec_hid, dec_cell = decoder(
                dec_inp, dec_hid, dec_cell, enc_out
            )

            best = decoder_output.argmax(1)  # shape (1,)
            result.append(de_idx2_word[best.item()])

            if best.item() == EOS:
                break

            dec_inp = best.unsqueeze(1)

    result = [w for w in result if w not in ['<EOS>', '<PAD>', '<SOS>']]
    val_outs.append(" ".join(result))

val_outs[0]

df0 = pd.DataFrame()
df0["ID"] = val_ids
df0["Translation"] = val_outs

df0.head()

# df0.to_csv('answersH.csv', index = False)

# x=pd.read_csv("/content/answersH.csv")

"""# Doing the above  thing for both language English-Bengali , English-Hindi ."""

df1 = pd.read_csv("/content/answersB.csv") # Bengali
df2= pd.read_csv("/content/answersH.csv")  # Hindi

df2.head()

df3 = pd.concat([df1, df2]) #Concat

df3

df3.to_csv('answersBH.csv', index = False)

filtered_data = pd.read_csv("/content/answersBH.csv")

answer = "/content/answer.csv"
with open(answer, "w") as f:
  f.writelines("ID\tTranslation\n")
  for i in range(filtered_data.shape[0]):
    f.writelines(f'{filtered_data["ID"][i]}\t"{filtered_data["Translation"][i]}"\n')